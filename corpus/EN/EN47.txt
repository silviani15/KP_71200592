Artificial Intelligence

Artificial intelligence is intelligence added to a manageable system in a scientific context or it can also be called artificial intelligence (English: artificial intelligence) or simply abbreviated as AI, defined as the intelligence of a scientific entity. Andreas Kaplan and Michael Haenlein define artificial intelligence as "the ability of a system to correctly interpret external data, to learn from that data, and to use that learning to accomplish specific goals and tasks through flexible adaptation". Such systems are generally considered computers. Intelligence is created and incorporated into computers so that they can do the work that humans can do. Some of the various fields that use artificial intelligence include expert systems, computer games, fuzzy logic, artificial neural networks and robotics. Technically, artificial intelligence is a statistical model used to make decisions by generalizing the characteristics of a data-driven object that is then installed in various electronic devices.

Although AI has strong science fiction connotations, it forms a very important branch of computer science, dealing with intelligent behavior, learning and adaptation in machines. Research in AI involves creating machines and computer programs to automate tasks that require intelligent behavior. Examples include controlling, planning and scheduling, the ability to answer diagnoses and customer questions, as well as handwriting, voice and facial recognition. These have become disciplines in their own right, focusing on providing solutions to real-life problems. AI systems are now frequently used in economics, science, medicine, engineering and the military, as they have been built into some home computer software applications and video games.

Broadly speaking, AI is divided into two schools of thought namely Conventional AI and Computational Intelligence (CI, Computational Intelligence). Conventional AI mostly involves methods now classified as machine learning, characterized by formalism and statistical analysis. Also known as symbolic AI, logical AI, pure AI and Good Old Fashioned Artificial Intelligence (GOFAI).

During the 1960s and 1970s, Joel Moses demonstrated the power of symbolic reasoning to integrate problems in Macsyma, the first successful knowledge-based program in mathematics. Marvin Minsky and Seymour Papert published Perceptrons, which demonstrated the limits of simple neural networks and Alain Colmerauer developed the Prolog computer language. Ted Shortliffe demonstrated the power of rule-based systems for knowledge representation and inference in medical diagnosis and therapy, sometimes referred to as the first expert system. Hans Moravec developed the first computer-controlled vehicle for autonomously navigating tangled interstates.

In the 1980s, neural networks were widely used with back propagation algorithms, first described by Paul John Werbos in 1974. In 1982, physicists such as Hopfield used statistical techniques to analyze the storage and optimization properties of neural networks. Psychologists David Rumelhart and Geoff Hinton continued research on neural network models of memory. In the 1985s at least four research groups reinvented the back propagation learning algorithm. These algorithms were successfully implemented into computer science and psychology. The 1990s saw major gains in various fields of AI and demonstrations of various applications. More specifically Deep Blue, a chess-playing computer, defeated Garry Kasparov in a famous 6-game match in 1997. DARPA stated that the costs saved through the application of AI methods to unit scheduling in the first Gulf War had reimbursed the US government for its entire investment in AI research since 1950.